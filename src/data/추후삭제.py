import json
import google.generativeai as genai
import time
import os
from typing import List, Dict, Any
from datetime import datetime

# Gemini API ì„¤ì •
GEMINI_API_KEY = "AIzaSyDuYkCYdaSeR_M-ADD8ylwmpKWbku61u00"  # ì—¬ê¸°ì— ì‹¤ì œ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”
genai.configure(api_key=GEMINI_API_KEY)

# í™•ì¥ëœ ì¹´í…Œê³ ë¦¬ ì •ì˜
CATEGORIES = [
    "í•™êµÂ·êµìœ¡",
    "ì§ì¥Â·ë¹„ì¦ˆë‹ˆìŠ¤", 
    "ì—¬í–‰Â·êµí†µ",
    "ìŒì‹Â·ìš”ë¦¬",
    "ê±´ê°•Â·ë³‘ì›",
    "ê°ì •Â·ì„±ê²©",
    "ìì—°Â·ë‚ ì”¨",
    "ì¼ìƒìƒí™œÂ·ê°€ì •",  # ìƒˆë¡œ ì¶”ê°€ëœ ì¹´í…Œê³ ë¦¬
    "ê¸°íƒ€"
]

# ì„¤ì •
CONFIG = {
    "batch_size": 10,  # ê¸°íƒ€ ì¬ë¶„ë¥˜ìš©ìœ¼ë¡œ ë” ì‘ê²Œ ì„¤ì •
    "delay": 2,        # API í˜¸ì¶œ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
    "save_interval": 3,  # ëª‡ ë°°ì¹˜ë§ˆë‹¤ ì¤‘ê°„ ì €ì¥í• ì§€
    "backup_folder": "backup_recategorization"  # ë°±ì—… í´ë”ëª…
}

class VocabularyRecategorizer:
    def __init__(self):
        self.model = genai.GenerativeModel('gemini-1.5-flash')
        self.batch_size = CONFIG["batch_size"]
        self.delay = CONFIG["delay"]
        self.save_interval = CONFIG["save_interval"]
        self.backup_folder = CONFIG["backup_folder"]
        
        # ë°±ì—… í´ë” ìƒì„±
        if not os.path.exists(self.backup_folder):
            os.makedirs(self.backup_folder)
    
    def create_progress_file(self, output_file: str) -> str:
        """ì§„í–‰ìƒí™© íŒŒì¼ ê²½ë¡œ ìƒì„±"""
        base_name = os.path.splitext(os.path.basename(output_file))[0]
        return os.path.join(self.backup_folder, f"{base_name}_recategorize_progress.json")
    
    def save_progress(self, vocab_data: List[Dict], etc_indices: List[int], progress_file: str, current_batch: int, total_batches: int):
        """ì§„í–‰ìƒí™© ì €ì¥"""
        progress_info = {
            "timestamp": datetime.now().isoformat(),
            "current_batch": current_batch,
            "total_batches": total_batches,
            "processed_count": sum(1 for i in etc_indices if vocab_data[i].get('category') != 'ê¸°íƒ€'),
            "total_etc_count": len(etc_indices)
        }
        
        # ë°ì´í„°ì™€ ì§„í–‰ìƒí™©ì„ í•¨ê»˜ ì €ì¥
        backup_data = {
            "progress": progress_info,
            "vocab_data": vocab_data,
            "etc_indices": etc_indices
        }
        
        with open(progress_file, 'w', encoding='utf-8') as f:
            json.dump(backup_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ì§„í–‰ìƒí™© ì €ì¥ë¨: ë°°ì¹˜ {current_batch}/{total_batches}")
    
    def load_progress(self, progress_file: str) -> tuple:
        """ì§„í–‰ìƒí™© ë¡œë“œ"""
        if not os.path.exists(progress_file):
            return None, None, 0
        
        try:
            with open(progress_file, 'r', encoding='utf-8') as f:
                backup_data = json.load(f)
            
            vocab_data = backup_data.get("vocab_data", [])
            etc_indices = backup_data.get("etc_indices", [])
            progress = backup_data.get("progress", {})
            current_batch = progress.get("current_batch", 0)
            
            print(f"ğŸ“‚ ì´ì „ ì§„í–‰ìƒí™© ë°œê²¬:")
            print(f"   - ë§ˆì§€ë§‰ ì²˜ë¦¬ ë°°ì¹˜: {current_batch}")
            print(f"   - ì¬ë¶„ë¥˜ëœ ì–´íœ˜: {progress.get('processed_count', 0)}ê°œ")
            print(f"   - ì´ ê¸°íƒ€ ì–´íœ˜: {progress.get('total_etc_count', 0)}ê°œ")
            print(f"   - ë§ˆì§€ë§‰ ì €ì¥ì‹œê°„: {progress.get('timestamp', 'Unknown')}")
            
            return vocab_data, etc_indices, current_batch
            
        except Exception as e:
            print(f"âš ï¸ ì§„í–‰ìƒí™© ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None, None, 0
    
    def find_etc_vocabulary(self, vocab_data: List[Dict]) -> List[int]:
        """ê¸°íƒ€ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜ëœ ì–´íœ˜ë“¤ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°"""
        etc_indices = []
        for i, vocab in enumerate(vocab_data):
            if vocab.get('category') == 'ê¸°íƒ€':
                etc_indices.append(i)
        return etc_indices
    
    def create_recategorization_prompt(self, vocab_batch: List[Dict]) -> str:
        """ê¸°íƒ€ ì¬ë¶„ë¥˜ìš© ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        vocab_list = ""
        for i, vocab in enumerate(vocab_batch, 1):
            simplified = vocab.get('simplified', '')
            meaning = vocab.get('meaning', {}).get('ko', '')
            example_zh = vocab.get('example', {}).get('zh', '')
            example_ko = vocab.get('example', {}).get('ko', '')
            
            vocab_list += f"{i}. {simplified} - {meaning}\n"
            if example_zh and example_ko:
                vocab_list += f"   ì˜ˆë¬¸: {example_zh} ({example_ko})\n"
            vocab_list += "\n"
        
        prompt = f"""
ë‹¤ìŒ ì¤‘êµ­ì–´ ì–´íœ˜ë“¤ì„ ì•„ë˜ 9ê°œ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•´ì£¼ì„¸ìš”. 
ì´ ì–´íœ˜ë“¤ì€ ì´ì „ì— "ê¸°íƒ€"ë¡œ ë¶„ë¥˜ë˜ì—ˆì§€ë§Œ, ë” êµ¬ì²´ì ì¸ ì¹´í…Œê³ ë¦¬ë¡œ ì¬ë¶„ë¥˜í•˜ë ¤ê³  í•©ë‹ˆë‹¤.

ì¹´í…Œê³ ë¦¬:
1. í•™êµÂ·êµìœ¡ - í•™êµ, ê³µë¶€, êµìœ¡, í•™ìŠµ, ì‹œí—˜, ì±…, ì—°í•„, êµì‹¤, ì„ ìƒë‹˜, í•™ìƒ ê´€ë ¨
2. ì§ì¥Â·ë¹„ì¦ˆë‹ˆìŠ¤ - íšŒì‚¬, ì—…ë¬´, ê²½ì œ, ê¸ˆìœµ, ì‚¬ë¬´ìš©í’ˆ, íšŒì˜, ê³„ì•½, ëˆ, ì€í–‰ ê´€ë ¨
3. ì—¬í–‰Â·êµí†µ - ì—¬í–‰, êµí†µìˆ˜ë‹¨, ìˆ™ë°•, ê´€ê´‘, ë¹„í–‰ê¸°, ê¸°ì°¨, ë²„ìŠ¤, í˜¸í…”, ê³µí•­ ê´€ë ¨
4. ìŒì‹Â·ìš”ë¦¬ - ìŒì‹, ìš”ë¦¬, ì‹ë‹¹, ì‹ì¬ë£Œ, ë§›, ì£¼ë°©ìš©í’ˆ, ìŒë£Œ, ê³¼ì¼, ì•¼ì±„ ê´€ë ¨
5. ê±´ê°•Â·ë³‘ì› - ê±´ê°•, ì˜ë£Œ, ë³‘ì›, ì•½í’ˆ, ì¦ìƒ, ì¹˜ë£Œ, ìš´ë™, ëª¸, ì˜ì‚¬ ê´€ë ¨
6. ê°ì •Â·ì„±ê²© - ê°ì •, ì„±ê²©, ì‹¬ë¦¬ìƒíƒœ, ê¸°ë¶„, ì„±í–¥, íƒœë„, ëŠë‚Œ ê´€ë ¨
7. ìì—°Â·ë‚ ì”¨ - ìì—°, ë‚ ì”¨, í™˜ê²½, ë™ì‹ë¬¼, ê³„ì ˆ, ë°”ëŒ, ë¹„, ì‚°, ë°”ë‹¤, ê½ƒ ê´€ë ¨
8. ì¼ìƒìƒí™œÂ·ê°€ì • - ì§‘, ê°€ì¡±, ì¼ìƒìš©í’ˆ, ìƒí™œìš©í’ˆ, ê°€êµ¬, ì „ìì œí’ˆ, ì˜·, ì²­ì†Œ, ìš”ë¦¬ë„êµ¬, ì¹¨ì‹¤, ê±°ì‹¤ ê´€ë ¨
9. ê¸°íƒ€ - ìœ„ 8ê°œ ì¹´í…Œê³ ë¦¬ì— ì •ë§ë¡œ í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ë§¤ìš° ì¶”ìƒì ì´ê±°ë‚˜ íŠ¹ìˆ˜í•œ ë‹¨ì–´

ë¶„ë¥˜í•  ì–´íœ˜:
{vocab_list}

ë¶„ë¥˜ ì§€ì¹¨:
- ë‹¨ì–´ì˜ í•µì‹¬ ì˜ë¯¸ì™€ ê°€ì¥ ì¼ë°˜ì ì¸ ì‚¬ìš© ë§¥ë½ì„ ê³ ë ¤í•˜ì„¸ìš”
- ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•  ìˆ˜ ìˆë‹¤ë©´, ê°€ì¥ ëŒ€í‘œì ì¸ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”
- "ì¼ìƒìƒí™œÂ·ê°€ì •" ì¹´í…Œê³ ë¦¬ë¥¼ ì ê·¹ í™œìš©í•˜ì„¸ìš” (ê°€ì •ìš©í’ˆ, ìƒí™œìš©í’ˆ, ì˜ë¥˜, ê°€êµ¬ ë“±)
- "ê¸°íƒ€"ëŠ” ì •ë§ë¡œ ë¶„ë¥˜ê°€ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°ì—ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
- êµ¬ì²´ì ì¸ ì‚¬ë¬¼ì´ë‚˜ í–‰ë™ì€ ê°€ëŠ¥í•œ í•œ êµ¬ì²´ì  ì¹´í…Œê³ ë¦¬ì— ë°°ì •í•˜ì„¸ìš”

ì‘ë‹µ í˜•ì‹: ê° ì–´íœ˜ ë²ˆí˜¸ì™€ ì¹´í…Œê³ ë¦¬ëª…ì„ ì •í™•íˆ ë§¤ì¹­í•´ì„œ ë‹¤ìŒê³¼ ê°™ì´ ë‹µí•´ì£¼ì„¸ìš”:
1: ì¹´í…Œê³ ë¦¬ëª…
2: ì¹´í…Œê³ ë¦¬ëª…
3: ì¹´í…Œê³ ë¦¬ëª…
...

ì£¼ì˜ì‚¬í•­:
- ë°˜ë“œì‹œ ìœ„ì˜ 9ê°œ ì¹´í…Œê³ ë¦¬ ì¤‘ì—ì„œë§Œ ì„ íƒí•˜ì„¸ìš”
- ëª¨ë“  ì–´íœ˜ì— ëŒ€í•´ ì‘ë‹µí•´ì£¼ì„¸ìš”
- ì¹´í…Œê³ ë¦¬ëª…ì€ ì •í™•íˆ ìœ„ì— ì œì‹œëœ ì´ë¦„ê³¼ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤
"""
        return prompt
    
    def parse_gemini_response(self, response: str, vocab_batch: List[Dict]) -> List[str]:
        """Gemini ì‘ë‹µì„ íŒŒì‹±í•´ì„œ ì¹´í…Œê³ ë¦¬ ëª©ë¡ ë°˜í™˜"""
        categories = []
        lines = response.strip().split('\n')
        
        for line in lines:
            line = line.strip()
            if ':' in line:
                try:
                    # "1: í•™êµÂ·êµìœ¡" í˜•íƒœì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
                    category = line.split(':', 1)[1].strip()
                    if category in CATEGORIES:
                        categories.append(category)
                    else:
                        print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì¹´í…Œê³ ë¦¬: {category}")
                        categories.append("ê¸°íƒ€")
                except:
                    categories.append("ê¸°íƒ€")
        
        # ì‘ë‹µì´ ë¶€ì¡±í•œ ê²½ìš° "ê¸°íƒ€"ë¡œ ì±„ìš°ê¸°
        while len(categories) < len(vocab_batch):
            categories.append("ê¸°íƒ€")
            
        return categories[:len(vocab_batch)]
    
    def categorize_batch(self, vocab_batch: List[Dict]) -> List[str]:
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì–´íœ˜ ì¹´í…Œê³ ë¦¬ ì¬ë¶„ë¥˜"""
        try:
            prompt = self.create_recategorization_prompt(vocab_batch)
            response = self.model.generate_content(prompt)
            
            if response.text:
                categories = self.parse_gemini_response(response.text, vocab_batch)
                return categories
            else:
                print("âš ï¸ Gemini ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return ["ê¸°íƒ€"] * len(vocab_batch)
                
        except Exception as e:
            print(f"âŒ Gemini API ì˜¤ë¥˜: {e}")
            return ["ê¸°íƒ€"] * len(vocab_batch)
    
    def recategorize_etc_vocabulary(self, input_file: str, output_file: str):
        """ê¸°íƒ€ ì¹´í…Œê³ ë¦¬ ì–´íœ˜ë“¤ë§Œ ì¬ë¶„ë¥˜"""
        print(f"ğŸ“‚ íŒŒì¼ ë¡œë”©: {input_file}")
        
        # ì§„í–‰ìƒí™© íŒŒì¼ ê²½ë¡œ
        progress_file = self.create_progress_file(output_file)
        
        # ì´ì „ ì§„í–‰ìƒí™© í™•ì¸
        vocab_data, etc_indices, start_batch = self.load_progress(progress_file)
        
        if vocab_data is None:
            # ìƒˆë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°
            with open(input_file, 'r', encoding='utf-8') as f:
                vocab_data = json.load(f)
            
            # ê¸°íƒ€ ì¹´í…Œê³ ë¦¬ ì–´íœ˜ë“¤ ì°¾ê¸°
            etc_indices = self.find_etc_vocabulary(vocab_data)
            start_batch = 0
            
            print(f"ğŸ“Š ë¶„ì„ ê²°ê³¼:")
            print(f"   - ì „ì²´ ì–´íœ˜ ìˆ˜: {len(vocab_data)}ê°œ")
            print(f"   - ê¸°íƒ€ ì¹´í…Œê³ ë¦¬ ì–´íœ˜: {len(etc_indices)}ê°œ")
            print(f"   - ì¬ë¶„ë¥˜ ë¹„ìœ¨: {len(etc_indices)/len(vocab_data)*100:.1f}%")
            
            if len(etc_indices) == 0:
                print("âœ… ê¸°íƒ€ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜ëœ ì–´íœ˜ê°€ ì—†ìŠµë‹ˆë‹¤!")
                return
        else:
            print(f"ğŸ”„ ì´ì „ ì‘ì—… ì´ì–´ì„œ ì§„í–‰ - ê¸°íƒ€ ì–´íœ˜: {len(etc_indices)}ê°œ")
            
            # ê³„ì† ì§„í–‰í• ì§€ í™•ì¸
            response = input("ì´ì „ ì‘ì—…ì„ ì´ì–´ì„œ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
            if response.lower() != 'y':
                print("âŒ ì‘ì—…ì„ ì·¨ì†Œí•©ë‹ˆë‹¤.")
                return
        
        # ë°°ì¹˜ ê³„ì‚°
        total_batches = (len(etc_indices) + self.batch_size - 1) // self.batch_size
        
        print(f"\nğŸ“Š ì¬ë¶„ë¥˜ ì‘ì—… ì •ë³´:")
        print(f"   - ì²˜ë¦¬í•  ì–´íœ˜ ìˆ˜: {len(etc_indices)}ê°œ")
        print(f"   - ì´ ë°°ì¹˜ ìˆ˜: {total_batches}ê°œ")
        print(f"   - ë°°ì¹˜ë‹¹ ì–´íœ˜ ìˆ˜: {self.batch_size}ê°œ")
        print(f"   - ë°°ì¹˜ê°„ ëŒ€ê¸°ì‹œê°„: {self.delay}ì´ˆ")
        
        # ì‹œì‘ í™•ì¸
        if start_batch == 0:
            print(f"\nğŸ“‹ í™•ì¥ëœ ì¹´í…Œê³ ë¦¬:")
            for i, cat in enumerate(CATEGORIES, 1):
                if cat == "ì¼ìƒìƒí™œÂ·ê°€ì •":
                    print(f"   {i}. {cat} â­ (ìƒˆë¡œ ì¶”ê°€ë¨)")
                else:
                    print(f"   {i}. {cat}")
            
            response = input("\nê¸°íƒ€ ì¹´í…Œê³ ë¦¬ ì¬ë¶„ë¥˜ ì‘ì—…ì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
            if response.lower() != 'y':
                print("âŒ ì‘ì—…ì„ ì·¨ì†Œí•©ë‹ˆë‹¤.")
                return
        
        # ë°°ì¹˜ë³„ë¡œ ì²˜ë¦¬
        recategorized_count = 0
        
        try:
            for batch_idx in range(start_batch, total_batches):
                start_idx = batch_idx * self.batch_size
                end_idx = min(start_idx + self.batch_size, len(etc_indices))
                
                # í˜„ì¬ ë°°ì¹˜ì˜ ì–´íœ˜ë“¤ ê°€ì ¸ì˜¤ê¸°
                batch_vocab_indices = etc_indices[start_idx:end_idx]
                batch_vocab = [vocab_data[i] for i in batch_vocab_indices]
                
                print(f"\nğŸ”„ ë°°ì¹˜ {batch_idx + 1}/{total_batches} ì²˜ë¦¬ ì¤‘... ({len(batch_vocab)}ê°œ ì–´íœ˜)")
                
                # ì–´íœ˜ ë¯¸ë¦¬ë³´ê¸°
                print("   ğŸ“ ì²˜ë¦¬í•  ì–´íœ˜ë“¤:")
                for vocab in batch_vocab[:3]:  # ì²˜ìŒ 3ê°œë§Œ ë¯¸ë¦¬ë³´ê¸°
                    simplified = vocab.get('simplified', '')
                    meaning = vocab.get('meaning', {}).get('ko', '')
                    print(f"      - {simplified} ({meaning})")
                if len(batch_vocab) > 3:
                    print(f"      ... ì™¸ {len(batch_vocab)-3}ê°œ")
                
                # ì¹´í…Œê³ ë¦¬ ì¬ë¶„ë¥˜
                new_categories = self.categorize_batch(batch_vocab)
                
                # ê²°ê³¼ ì ìš© ë° í†µê³„
                batch_stats = {}
                for i, new_category in enumerate(new_categories):
                    vocab_data[batch_vocab_indices[i]]['category'] = new_category
                    batch_stats[new_category] = batch_stats.get(new_category, 0) + 1
                    if new_category != 'ê¸°íƒ€':
                        recategorized_count += 1
                
                # ë°°ì¹˜ ê²°ê³¼ ì¶œë ¥
                print(f"   âœ… ë°°ì¹˜ ê²°ê³¼:")
                for cat, count in batch_stats.items():
                    if cat != 'ê¸°íƒ€':
                        print(f"      - {cat}: {count}ê°œ â­")
                    else:
                        print(f"      - {cat}: {count}ê°œ")
                
                # ì „ì²´ ì§„í–‰ìƒí™© ì¶œë ¥
                processed_batches = batch_idx + 1
                total_processed = processed_batches * self.batch_size
                if total_processed > len(etc_indices):
                    total_processed = len(etc_indices)
                
                print(f"ğŸ“Š ì „ì²´ ì§„í–‰ìƒí™©: {total_processed}/{len(etc_indices)} ({total_processed/len(etc_indices)*100:.1f}%)")
                print(f"ğŸ¯ ì¬ë¶„ë¥˜ ì„±ê³µ: {recategorized_count}ê°œ")
                
                # ì¤‘ê°„ ì €ì¥
                if (batch_idx + 1) % self.save_interval == 0 or batch_idx == total_batches - 1:
                    self.save_progress(vocab_data, etc_indices, progress_file, batch_idx + 1, total_batches)
                
                # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ëŒ€ê¸°
                if batch_idx < total_batches - 1:  # ë§ˆì§€ë§‰ ë°°ì¹˜ê°€ ì•„ë‹Œ ê²½ìš°
                    print(f"â³ {self.delay}ì´ˆ ëŒ€ê¸° ì¤‘...")
                    time.sleep(self.delay)
        
        except KeyboardInterrupt:
            print(f"\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print(f"ğŸ’¾ í˜„ì¬ê¹Œì§€ì˜ ì§„í–‰ìƒí™©ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {progress_file}")
            print(f"ğŸ”„ ë‹¤ì‹œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ë©´ ì´ì–´ì„œ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            return
        
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            self.save_progress(vocab_data, etc_indices, progress_file, batch_idx, total_batches)
            print(f"ğŸ’¾ ì˜¤ë¥˜ ë°œìƒ ì „ê¹Œì§€ì˜ ì§„í–‰ìƒí™©ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return
        
        # ìµœì¢… ê²°ê³¼ ì €ì¥
        print(f"\nğŸ’¾ ìµœì¢… ê²°ê³¼ ì €ì¥: {output_file}")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(vocab_data, f, ensure_ascii=False, indent=2)
        
        # ë°±ì—… íŒŒì¼ë„ ì €ì¥
        backup_final = os.path.join(self.backup_folder, f"final_recategorized_{os.path.basename(output_file)}")
        with open(backup_final, 'w', encoding='utf-8') as f:
            json.dump(vocab_data, f, ensure_ascii=False, indent=2)
        
        # ì¬ë¶„ë¥˜ ê²°ê³¼ í†µê³„
        final_category_stats = {}
        for vocab in vocab_data:
            category = vocab.get('category', 'ë¯¸ë¶„ë¥˜')
            final_category_stats[category] = final_category_stats.get(category, 0) + 1
        
        print("\nğŸ“ˆ ìµœì¢… ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜ ê²°ê³¼:")
        for category, count in sorted(final_category_stats.items(), key=lambda x: x[1], reverse=True):
            if category == "ê¸°íƒ€":
                print(f"  {category}: {count}ê°œ (ì¬ë¶„ë¥˜ í›„)")
            else:
                print(f"  {category}: {count}ê°œ")
        
        # ì¬ë¶„ë¥˜ ì„±ê³¼ ìš”ì•½
        remaining_etc = final_category_stats.get('ê¸°íƒ€', 0)
        original_etc = len(etc_indices)
        success_rate = (recategorized_count / original_etc * 100) if original_etc > 0 else 0
        
        print(f"\nğŸ‰ ì¬ë¶„ë¥˜ ì™„ë£Œ!")
        print(f"  ğŸ“Š ì›ë˜ ê¸°íƒ€: {original_etc}ê°œ")
        print(f"  âœ… ì¬ë¶„ë¥˜ ì„±ê³µ: {recategorized_count}ê°œ")
        print(f"  ğŸ“‰ ë‚¨ì€ ê¸°íƒ€: {remaining_etc}ê°œ") 
        print(f"  ğŸ¯ ì„±ê³µë¥ : {success_rate:.1f}%")
        
        # ì§„í–‰ìƒí™© íŒŒì¼ ì •ë¦¬
        if os.path.exists(progress_file):
            os.remove(progress_file)
            print(f"ğŸ—‘ï¸ ì§„í–‰ìƒí™© íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")
        
        print(f"ğŸ“ ë°±ì—… íŒŒì¼: {backup_final}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”„ ì¤‘êµ­ì–´ ì–´íœ˜ ê¸°íƒ€ ì¹´í…Œê³ ë¦¬ ì¬ë¶„ë¥˜ê¸°")
    print("=" * 60)
    
    # API í‚¤ í™•ì¸
    if GEMINI_API_KEY == "YOUR_GEMINI_API_KEY_HERE":
        print("âŒ Gemini API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!")
        print("ì½”ë“œ ìƒë‹¨ì˜ GEMINI_API_KEY ë³€ìˆ˜ì— ì‹¤ì œ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        return
    
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    input_file = r"C:\Users\jisun\Desktop\firebase_project\3_word\chinese-learning-project-1\src\data\merged_vocab_categorized.json"
    output_file = "merged_vocab_recategorized.json"  # ì¬ë¶„ë¥˜ ê²°ê³¼ íŒŒì¼
    
    # ì…ë ¥ íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(input_file):
        print(f"âŒ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
        print("íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    
    # ì¬ë¶„ë¥˜ê¸° ì‹¤í–‰
    recategorizer = VocabularyRecategorizer()
    
    try:
        recategorizer.recategorize_etc_vocabulary(input_file, output_file)
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()