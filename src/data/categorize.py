import json
import google.generativeai as genai
import time
import os
from typing import List, Dict, Any
from datetime import datetime

# Gemini API ì„¤ì •
GEMINI_API_KEY = "AIzaSyDuYkCYdaSeR_M-ADD8ylwmpKWbku61u00"  # ì—¬ê¸°ì— ì‹¤ì œ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”
genai.configure(api_key=GEMINI_API_KEY)

# ì¹´í…Œê³ ë¦¬ ì •ì˜
CATEGORIES = [
    "í•™êµÂ·êµìœ¡",
    "ì§ì¥Â·ë¹„ì¦ˆë‹ˆìŠ¤", 
    "ì—¬í–‰Â·êµí†µ",
    "ìŒì‹Â·ìš”ë¦¬",
    "ê±´ê°•Â·ë³‘ì›",
    "ê°ì •Â·ì„±ê²©",
    "ìì—°Â·ë‚ ì”¨",
    "ê¸°íƒ€"
]

# ì„¤ì •
CONFIG = {
    "batch_size": 15,  # í•œ ë²ˆì— ì²˜ë¦¬í•  ì–´íœ˜ ìˆ˜ (ì•ˆì „í•˜ê²Œ ì¤„ì„)
    "delay": 2,        # API í˜¸ì¶œ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
    "save_interval": 5,  # ëª‡ ë°°ì¹˜ë§ˆë‹¤ ì¤‘ê°„ ì €ì¥í• ì§€
    "backup_folder": "backup_categorization"  # ë°±ì—… í´ë”ëª…
}

class VocabularyCategorizer:
    def __init__(self):
        self.model = genai.GenerativeModel('gemini-1.5-flash')
        self.batch_size = CONFIG["batch_size"]
        self.delay = CONFIG["delay"]
        self.save_interval = CONFIG["save_interval"]
        self.backup_folder = CONFIG["backup_folder"]
        
        # ë°±ì—… í´ë” ìƒì„±
        if not os.path.exists(self.backup_folder):
            os.makedirs(self.backup_folder)
    
    def create_progress_file(self, output_file: str) -> str:
        """ì§„í–‰ìƒí™© íŒŒì¼ ê²½ë¡œ ìƒì„±"""
        base_name = os.path.splitext(os.path.basename(output_file))[0]
        return os.path.join(self.backup_folder, f"{base_name}_progress.json")
    
    def save_progress(self, vocab_data: List[Dict], progress_file: str, current_batch: int, total_batches: int):
        """ì§„í–‰ìƒí™© ì €ì¥"""
        progress_info = {
            "timestamp": datetime.now().isoformat(),
            "current_batch": current_batch,
            "total_batches": total_batches,
            "processed_count": sum(1 for vocab in vocab_data if 'category' in vocab),
            "total_count": len(vocab_data)
        }
        
        # ë°ì´í„°ì™€ ì§„í–‰ìƒí™©ì„ í•¨ê»˜ ì €ì¥
        backup_data = {
            "progress": progress_info,
            "vocab_data": vocab_data
        }
        
        with open(progress_file, 'w', encoding='utf-8') as f:
            json.dump(backup_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ì§„í–‰ìƒí™© ì €ì¥ë¨: ë°°ì¹˜ {current_batch}/{total_batches}")
    
    def load_progress(self, progress_file: str) -> tuple:
        """ì§„í–‰ìƒí™© ë¡œë“œ"""
        if not os.path.exists(progress_file):
            return None, 0
        
        try:
            with open(progress_file, 'r', encoding='utf-8') as f:
                backup_data = json.load(f)
            
            vocab_data = backup_data.get("vocab_data", [])
            progress = backup_data.get("progress", {})
            current_batch = progress.get("current_batch", 0)
            
            print(f"ğŸ“‚ ì´ì „ ì§„í–‰ìƒí™© ë°œê²¬:")
            print(f"   - ë§ˆì§€ë§‰ ì²˜ë¦¬ ë°°ì¹˜: {current_batch}")
            print(f"   - ì²˜ë¦¬ëœ ì–´íœ˜: {progress.get('processed_count', 0)}ê°œ")
            print(f"   - ë§ˆì§€ë§‰ ì €ì¥ì‹œê°„: {progress.get('timestamp', 'Unknown')}")
            
            return vocab_data, current_batch
            
        except Exception as e:
            print(f"âš ï¸ ì§„í–‰ìƒí™© ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None, 0
    
    def estimate_cost_and_time(self, total_vocab: int) -> dict:
        """ë¹„ìš© ë° ì‹œê°„ ì˜ˆìƒ"""
        total_batches = (total_vocab + self.batch_size - 1) // self.batch_size
        
        # Gemini 1.5 Flash ì˜ˆìƒ ë¹„ìš© (2024ë…„ ê¸°ì¤€, ì‹¤ì œì™€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
        estimated_tokens_per_batch = 1000  # ëŒ€ëµì ì¸ í† í° ìˆ˜
        total_tokens = total_batches * estimated_tokens_per_batch
        
        # ì‹œê°„ ì˜ˆìƒ (ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„ + ëŒ€ê¸° ì‹œê°„)
        estimated_time_minutes = (total_batches * (3 + self.delay)) / 60
        
        return {
            "total_batches": total_batches,
            "estimated_time_minutes": round(estimated_time_minutes, 1),
            "estimated_tokens": total_tokens
        }
        
    def create_categorization_prompt(self, vocab_batch: List[Dict]) -> str:
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        vocab_list = ""
        for i, vocab in enumerate(vocab_batch, 1):
            simplified = vocab.get('simplified', '')
            meaning = vocab.get('meaning', {}).get('ko', '')
            example_zh = vocab.get('example', {}).get('zh', '')
            example_ko = vocab.get('example', {}).get('ko', '')
            
            vocab_list += f"{i}. {simplified} - {meaning}\n"
            if example_zh and example_ko:
                vocab_list += f"   ì˜ˆë¬¸: {example_zh} ({example_ko})\n"
            vocab_list += "\n"
        
        prompt = f"""
ë‹¤ìŒ ì¤‘êµ­ì–´ ì–´íœ˜ë“¤ì„ ì•„ë˜ 8ê°œ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•´ì£¼ì„¸ìš”:

ì¹´í…Œê³ ë¦¬:
1. í•™êµÂ·êµìœ¡ - í•™êµ, ê³µë¶€, êµìœ¡, í•™ìŠµ ê´€ë ¨
2. ì§ì¥Â·ë¹„ì¦ˆë‹ˆìŠ¤ - íšŒì‚¬, ì—…ë¬´, ê²½ì œ, ê¸ˆìœµ ê´€ë ¨  
3. ì—¬í–‰Â·êµí†µ - ì—¬í–‰, êµí†µìˆ˜ë‹¨, ìˆ™ë°•, ê´€ê´‘ ê´€ë ¨
4. ìŒì‹Â·ìš”ë¦¬ - ìŒì‹, ìš”ë¦¬, ì‹ë‹¹, ì‹ì¬ë£Œ ê´€ë ¨
5. ê±´ê°•Â·ë³‘ì› - ê±´ê°•, ì˜ë£Œ, ë³‘ì›, ì•½í’ˆ ê´€ë ¨
6. ê°ì •Â·ì„±ê²© - ê°ì •, ì„±ê²©, ì‹¬ë¦¬ìƒíƒœ ê´€ë ¨
7. ìì—°Â·ë‚ ì”¨ - ìì—°, ë‚ ì”¨, í™˜ê²½, ë™ì‹ë¬¼ ê´€ë ¨
8. ê¸°íƒ€ - ìœ„ ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ì¼ë°˜ì ì¸ ë‹¨ì–´

ë¶„ë¥˜í•  ì–´íœ˜:
{vocab_list}

ì‘ë‹µ í˜•ì‹: ê° ì–´íœ˜ ë²ˆí˜¸ì™€ ì¹´í…Œê³ ë¦¬ëª…ì„ ì •í™•íˆ ë§¤ì¹­í•´ì„œ ë‹¤ìŒê³¼ ê°™ì´ ë‹µí•´ì£¼ì„¸ìš”:
1: ì¹´í…Œê³ ë¦¬ëª…
2: ì¹´í…Œê³ ë¦¬ëª…
3: ì¹´í…Œê³ ë¦¬ëª…
...

ì£¼ì˜ì‚¬í•­:
- ë°˜ë“œì‹œ ìœ„ì˜ 8ê°œ ì¹´í…Œê³ ë¦¬ ì¤‘ì—ì„œë§Œ ì„ íƒí•˜ì„¸ìš”
- ì–´íœ˜ì˜ ì˜ë¯¸ì™€ ì‚¬ìš© ë§¥ë½ì„ ê³ ë ¤í•´ì„œ ê°€ì¥ ì ì ˆí•œ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”
- ëª¨ë“  ì–´íœ˜ì— ëŒ€í•´ ì‘ë‹µí•´ì£¼ì„¸ìš”
"""
        return prompt
    
    def parse_gemini_response(self, response: str, vocab_batch: List[Dict]) -> List[str]:
        """Gemini ì‘ë‹µì„ íŒŒì‹±í•´ì„œ ì¹´í…Œê³ ë¦¬ ëª©ë¡ ë°˜í™˜"""
        categories = []
        lines = response.strip().split('\n')
        
        for line in lines:
            line = line.strip()
            if ':' in line:
                try:
                    # "1: í•™êµÂ·êµìœ¡" í˜•íƒœì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
                    category = line.split(':', 1)[1].strip()
                    if category in CATEGORIES:
                        categories.append(category)
                    else:
                        print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì¹´í…Œê³ ë¦¬: {category}")
                        categories.append("ê¸°íƒ€")
                except:
                    categories.append("ê¸°íƒ€")
        
        # ì‘ë‹µì´ ë¶€ì¡±í•œ ê²½ìš° "ê¸°íƒ€"ë¡œ ì±„ìš°ê¸°
        while len(categories) < len(vocab_batch):
            categories.append("ê¸°íƒ€")
            
        return categories[:len(vocab_batch)]
    
    def categorize_batch(self, vocab_batch: List[Dict]) -> List[str]:
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì–´íœ˜ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        try:
            prompt = self.create_categorization_prompt(vocab_batch)
            response = self.model.generate_content(prompt)
            
            if response.text:
                categories = self.parse_gemini_response(response.text, vocab_batch)
                return categories
            else:
                print("âš ï¸ Gemini ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return ["ê¸°íƒ€"] * len(vocab_batch)
                
        except Exception as e:
            print(f"âŒ Gemini API ì˜¤ë¥˜: {e}")
            return ["ê¸°íƒ€"] * len(vocab_batch)
    
    def categorize_vocabulary_file(self, input_file: str, output_file: str):
        """ì „ì²´ ì–´íœ˜ íŒŒì¼ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        print(f"ğŸ“‚ íŒŒì¼ ë¡œë”©: {input_file}")
        
        # ì§„í–‰ìƒí™© íŒŒì¼ ê²½ë¡œ
        progress_file = self.create_progress_file(output_file)
        
        # ì´ì „ ì§„í–‰ìƒí™© í™•ì¸
        vocab_data, start_batch = self.load_progress(progress_file)
        
        if vocab_data is None:
            # ìƒˆë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°
            with open(input_file, 'r', encoding='utf-8') as f:
                vocab_data = json.load(f)
            start_batch = 0
            print(f"ğŸ“Š ìƒˆë¡œìš´ ì‘ì—… ì‹œì‘ - ì´ ì–´íœ˜ ìˆ˜: {len(vocab_data)}ê°œ")
        else:
            print(f"ğŸ”„ ì´ì „ ì‘ì—… ì´ì–´ì„œ ì§„í–‰ - ì´ ì–´íœ˜ ìˆ˜: {len(vocab_data)}ê°œ")
            
            # ê³„ì† ì§„í–‰í• ì§€ í™•ì¸
            response = input("ì´ì „ ì‘ì—…ì„ ì´ì–´ì„œ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
            if response.lower() != 'y':
                print("âŒ ì‘ì—…ì„ ì·¨ì†Œí•©ë‹ˆë‹¤.")
                return
        
        # ë¹„ìš© ë° ì‹œê°„ ì˜ˆìƒ
        estimates = self.estimate_cost_and_time(len(vocab_data))
        total_batches = estimates["total_batches"]
        
        print(f"\nğŸ“Š ì‘ì—… ì˜ˆìƒ ì •ë³´:")
        print(f"   - ì´ ë°°ì¹˜ ìˆ˜: {total_batches}ê°œ")
        print(f"   - ì˜ˆìƒ ì†Œìš” ì‹œê°„: {estimates['estimated_time_minutes']}ë¶„")
        print(f"   - ì˜ˆìƒ í† í° ì‚¬ìš©ëŸ‰: {estimates['estimated_tokens']:,}ê°œ")
        print(f"   - ë°°ì¹˜ë‹¹ ëŒ€ê¸° ì‹œê°„: {self.delay}ì´ˆ")
        print(f"   - ì¤‘ê°„ ì €ì¥ ì£¼ê¸°: {self.save_interval}ë°°ì¹˜ë§ˆë‹¤")
        
        # ì‹œì‘ í™•ì¸
        if start_batch == 0:
            response = input("\nì‘ì—…ì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
            if response.lower() != 'y':
                print("âŒ ì‘ì—…ì„ ì·¨ì†Œí•©ë‹ˆë‹¤.")
                return
        
        print(f"ğŸ”„ ë°°ì¹˜ í¬ê¸°: {self.batch_size}ê°œì”© ì²˜ë¦¬")
        
        # ê¸°ì¡´ì— categoryê°€ ìˆëŠ”ì§€ í™•ì¸ (ìƒˆ ì‘ì—…ì¸ ê²½ìš°ë§Œ)
        if start_batch == 0:
            existing_categories = sum(1 for vocab in vocab_data if 'category' in vocab)
            if existing_categories > 0:
                print(f"âœ… ì´ë¯¸ ì¹´í…Œê³ ë¦¬ê°€ ìˆëŠ” ì–´íœ˜: {existing_categories}ê°œ")
                response = input("ê¸°ì¡´ ì¹´í…Œê³ ë¦¬ë¥¼ ë®ì–´ì“°ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
                if response.lower() != 'y':
                    print("âŒ ì‘ì—…ì„ ì·¨ì†Œí•©ë‹ˆë‹¤.")
                    return
        
        # ë°°ì¹˜ë³„ë¡œ ì²˜ë¦¬
        processed_count = sum(1 for vocab in vocab_data if 'category' in vocab)
        
        try:
            for batch_idx in range(start_batch, total_batches):
                start_idx = batch_idx * self.batch_size
                end_idx = min(start_idx + self.batch_size, len(vocab_data))
                batch = vocab_data[start_idx:end_idx]
                
                print(f"\nğŸ”„ ë°°ì¹˜ {batch_idx + 1}/{total_batches} ì²˜ë¦¬ ì¤‘... ({start_idx+1}-{end_idx})")
                
                # ì´ë¯¸ ì¹´í…Œê³ ë¦¬ê°€ ìˆëŠ” ì–´íœ˜ëŠ” ê±´ë„ˆë›°ê¸° (ì„ íƒì‚¬í•­)
                batch_to_process = []
                batch_indices = []
                for i, vocab in enumerate(batch):
                    if 'category' not in vocab or vocab.get('category') == '':
                        batch_to_process.append(vocab)
                        batch_indices.append(start_idx + i)
                
                if batch_to_process:
                    print(f"   ğŸ“ ì‹¤ì œ ì²˜ë¦¬í•  ì–´íœ˜: {len(batch_to_process)}ê°œ")
                    
                    # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
                    categories = self.categorize_batch(batch_to_process)
                    
                    # ê²°ê³¼ ì ìš©
                    for i, category in enumerate(categories):
                        vocab_data[batch_indices[i]]['category'] = category
                        processed_count += 1
                else:
                    print(f"   â­ï¸ ì´ë¯¸ ì²˜ë¦¬ëœ ë°°ì¹˜ ê±´ë„ˆë›°ê¸°")
                
                # ì§„í–‰ìƒí™© ì¶œë ¥
                print(f"âœ… ì™„ë£Œ: {processed_count}/{len(vocab_data)} ({(processed_count/len(vocab_data)*100):.1f}%)")
                
                # ì¤‘ê°„ ì €ì¥
                if (batch_idx + 1) % self.save_interval == 0 or batch_idx == total_batches - 1:
                    self.save_progress(vocab_data, progress_file, batch_idx + 1, total_batches)
                
                # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ëŒ€ê¸°
                if batch_idx < total_batches - 1:  # ë§ˆì§€ë§‰ ë°°ì¹˜ê°€ ì•„ë‹Œ ê²½ìš°
                    print(f"â³ {self.delay}ì´ˆ ëŒ€ê¸° ì¤‘...")
                    time.sleep(self.delay)
        
        except KeyboardInterrupt:
            print(f"\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print(f"ğŸ’¾ í˜„ì¬ê¹Œì§€ì˜ ì§„í–‰ìƒí™©ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {progress_file}")
            print(f"ğŸ”„ ë‹¤ì‹œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ë©´ ì´ì–´ì„œ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            return
        
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            self.save_progress(vocab_data, progress_file, batch_idx, total_batches)
            print(f"ğŸ’¾ ì˜¤ë¥˜ ë°œìƒ ì „ê¹Œì§€ì˜ ì§„í–‰ìƒí™©ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return
        
        # ìµœì¢… ê²°ê³¼ ì €ì¥
        print(f"\nğŸ’¾ ìµœì¢… ê²°ê³¼ ì €ì¥: {output_file}")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(vocab_data, f, ensure_ascii=False, indent=2)
        
        # ë°±ì—… íŒŒì¼ë„ ì €ì¥
        backup_final = os.path.join(self.backup_folder, f"final_{os.path.basename(output_file)}")
        with open(backup_final, 'w', encoding='utf-8') as f:
            json.dump(vocab_data, f, ensure_ascii=False, indent=2)
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        category_stats = {}
        for vocab in vocab_data:
            category = vocab.get('category', 'ë¯¸ë¶„ë¥˜')
            category_stats[category] = category_stats.get(category, 0) + 1
        
        print("\nğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜ ê²°ê³¼:")
        for category, count in sorted(category_stats.items(), key=lambda x: x[1], reverse=True):
            print(f"  {category}: {count}ê°œ")
        
        # ì§„í–‰ìƒí™© íŒŒì¼ ì •ë¦¬
        if os.path.exists(progress_file):
            os.remove(progress_file)
            print(f"ğŸ—‘ï¸ ì§„í–‰ìƒí™© íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")
        
        print(f"\nğŸ‰ ì™„ë£Œ! ì´ {len(vocab_data)}ê°œ ì–´íœ˜ê°€ ë¶„ë¥˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ğŸ“ ë°±ì—… íŒŒì¼: {backup_final}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ì¤‘êµ­ì–´ ì–´íœ˜ ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜ê¸°")
    print("=" * 50)
    
    # API í‚¤ í™•ì¸
    if GEMINI_API_KEY == "YOUR_GEMINI_API_KEY_HERE":
        print("âŒ Gemini API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!")
        print("ì½”ë“œ ìƒë‹¨ì˜ GEMINI_API_KEY ë³€ìˆ˜ì— ì‹¤ì œ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        return
    
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    input_file = "merged_vocab.json"  # ì…ë ¥ íŒŒì¼ëª…
    output_file = "merged_vocab_categorized.json"  # ì¶œë ¥ íŒŒì¼ëª…
    
    # ì…ë ¥ íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(input_file):
        print(f"âŒ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
        print("íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    
    # ë¶„ë¥˜ê¸° ì‹¤í–‰
    categorizer = VocabularyCategorizer()
    
    try:
        categorizer.categorize_vocabulary_file(input_file, output_file)
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()